{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01233095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RNN\n",
    "\n",
    "The aim of this notebook is getting the concepts and key feature of the Recurencet Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a20110",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m==============================================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mVANILLA RNN — From Scratch in NumPy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    dL/dW = sum over time of gradients flowing back through the unrolled graph\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     27\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m8\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "VANILLA RNN — From Scratch in NumPy\n",
    "==============================================================================\n",
    "\n",
    "Architecture:\n",
    "                    y_t\n",
    "                     ↑\n",
    "                  [W_hy]\n",
    "                     ↑\n",
    "   h_{t-1} --→ [  h_t  ] --→ h_t  (passed to next step)\n",
    "                  ↑    ↑\n",
    "              [W_hh] [W_xh]\n",
    "                ↑      ↑\n",
    "           h_{t-1}    x_t\n",
    "\n",
    "Core equations (forward pass):\n",
    "    h_t = activation( W_xh · x_t  +  W_hh · h_{t-1}  +  b_h )\n",
    "    y_t = softmax(    W_hy · h_t  +  b_y )\n",
    "\n",
    "Core equations (backward pass — BPTT):\n",
    "    dL/dW = sum over time of gradients flowing back through the unrolled graph\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. ACTIVATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"d/dx tanh(x) = 1 - tanh²(x)\"\"\"\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Converts raw scores (logits) into probabilities that sum to 1.\n",
    "    Subtract max for numerical stability (avoids overflow in exp).\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a54de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "plt.plot(x, tanh(x), label=\"tanh\")\n",
    "plt.plot(x, tanh_derivative(x), label=\"tanh derivative\")\n",
    "plt.legend()\n",
    "plt.title(\"Tanh Activation and Its Derivative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    \"\"\"\n",
    "    A character-level RNN that learns to predict the next character.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size  : int — number of unique input tokens (vocabulary size)\n",
    "    hidden_size : int — number of hidden units (the \"memory\" capacity)\n",
    "    output_size : int — number of unique output tokens (often = input_size)\n",
    "    lr          : float — learning rate\n",
    "\n",
    "    Weight matrices\n",
    "    ---------------\n",
    "    W_xh : (hidden_size, input_size)  — input  → hidden\n",
    "    W_hh : (hidden_size, hidden_size) — hidden → hidden (recurrence!)\n",
    "    W_hy : (output_size, hidden_size) — hidden → output\n",
    "    b_h  : (hidden_size, 1)           — hidden bias\n",
    "    b_y  : (output_size, 1)           — output bias\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "\n",
    "        # --- Weight initialization (Xavier / sqrt scale) ---\n",
    "        scale = lambda fan_in: 1.0 / np.sqrt(fan_in)\n",
    "\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size)  * scale(input_size)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale(hidden_size)\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * scale(hidden_size)\n",
    "        self.b_h  = np.zeros((hidden_size, 1))\n",
    "        self.b_y  = np.zeros((output_size, 1))\n",
    "\n",
    "        # --- Adagrad memory (for adaptive learning rate) ---\n",
    "        self.mW_xh = np.zeros_like(self.W_xh)\n",
    "        self.mW_hh = np.zeros_like(self.W_hh)\n",
    "        self.mW_hy = np.zeros_like(self.W_hy)\n",
    "        self.mb_h  = np.zeros_like(self.b_h)\n",
    "        self.mb_y  = np.zeros_like(self.b_y)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2a. FORWARD PASS  (one full sequence)\n",
    "    # -----------------------------------------------------------------\n",
    "    def forward(self, inputs, targets, h_prev):\n",
    "        \"\"\"\n",
    "        Run the RNN forward through the sequence and compute loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs  : list of int — input token indices  [x_0, x_1, ..., x_{T-1}]\n",
    "        targets : list of int — target token indices  [x_1, x_2, ..., x_T]\n",
    "        h_prev  : (hidden_size, 1) — initial hidden state\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss       : scalar — cross-entropy loss over the sequence\n",
    "        cache      : dict   — everything needed for backward pass\n",
    "\n",
    "        Step-by-step for each time step t:\n",
    "        ┌─────────────────────────────────────────────────┐\n",
    "        │  1. One-hot encode x_t                          │\n",
    "        │  2. Compute raw hidden: z_t = W_xh·x_t          │\n",
    "        │                              + W_hh·h_{t-1}     │\n",
    "        │                              + b_h               │\n",
    "        │  3. Apply activation:  h_t = tanh(z_t)          │\n",
    "        │  4. Compute output:    o_t = W_hy·h_t + b_y     │\n",
    "        │  5. Compute probs:     p_t = softmax(o_t)       │\n",
    "        │  6. Accumulate loss:   L -= log(p_t[target_t])   │\n",
    "        └─────────────────────────────────────────────────┘\n",
    "        \"\"\"\n",
    "        xs, hs, zs, os, ps = {}, {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)  # h at t=-1 is the initial state\n",
    "        loss = 0.0\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            # --- One-hot encode input ---\n",
    "            xs[t] = np.zeros((self.W_xh.shape[1], 1))\n",
    "            xs[t][inputs[t]] = 1.0\n",
    "\n",
    "            # --- Raw hidden state (pre-activation) ---\n",
    "            #       ┌── from input ──┐  ┌── recurrence ──┐  ┌ bias ┐\n",
    "            zs[t] = self.W_xh @ xs[t] + self.W_hh @ hs[t-1] + self.b_h\n",
    "\n",
    "            # --- Activated hidden state ---\n",
    "            hs[t] = tanh(zs[t])\n",
    "\n",
    "            # --- Output logits ---\n",
    "            os[t] = self.W_hy @ hs[t] + self.b_y\n",
    "\n",
    "            # --- Probabilities ---\n",
    "            ps[t] = softmax(os[t])\n",
    "\n",
    "            # --- Cross-entropy loss ---\n",
    "            loss -= np.log(ps[t][targets[t], 0] + 1e-12)\n",
    "\n",
    "        cache = {'xs': xs, 'hs': hs, 'zs': zs, 'ps': ps}\n",
    "        return loss, cache\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2b. BACKWARD PASS  (Backpropagation Through Time — BPTT)\n",
    "    # -----------------------------------------------------------------\n",
    "    def backward(self, inputs, targets, cache):\n",
    "        \"\"\"\n",
    "        Compute gradients via BPTT.\n",
    "\n",
    "        The key insight: since h_t depends on h_{t-1}, which depends on\n",
    "        h_{t-2}, etc., gradients flow backward through ALL previous time\n",
    "        steps. This is what makes RNNs powerful (and prone to vanishing\n",
    "        gradients).\n",
    "\n",
    "        Gradient flow diagram (backward):\n",
    "\n",
    "        dL/dy_t → dL/do_t → dL/dh_t ─→ dL/dW_hy\n",
    "                                │\n",
    "                    ┌───────────┴───────────┐\n",
    "                    ↓                       ↓\n",
    "               dL/dz_t                 dL/dh_{t-1}  ← RECURRENT GRADIENT!\n",
    "                 │  │                       │\n",
    "                 ↓  ↓                       ↓\n",
    "          dL/dW_xh  dL/dW_hh          (flows to t-1)\n",
    "        \"\"\"\n",
    "        xs, hs, zs, ps = cache['xs'], cache['hs'], cache['zs'], cache['ps']\n",
    "\n",
    "        # Initialize gradients\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h  = np.zeros_like(self.b_h)\n",
    "        db_y  = np.zeros_like(self.b_y)\n",
    "\n",
    "        dh_next = np.zeros_like(hs[0])  # gradient flowing from future\n",
    "\n",
    "        # Walk BACKWARD through time\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            # --- Output gradient ---\n",
    "            # For cross-entropy + softmax: dL/do_t = p_t - one_hot(target)\n",
    "            do = np.copy(ps[t])\n",
    "            do[targets[t]] = do[targets[t]] - 1.0  # This elegant formula is softmax + CE combined\n",
    "            # ∂L/∂o = p - one_hot(c)\n",
    "\n",
    "            # --- Gradients for W_hy, b_y ---\n",
    "            dW_hy += do @ hs[t].T    # (output_size, hidden_size)\n",
    "            db_y  += do\n",
    "\n",
    "            # --- Hidden state gradient ---\n",
    "            # dL/dh_t comes from TWO sources:\n",
    "            #   1. The output at time t:     W_hy^T · do\n",
    "            #   2. The hidden state at t+1:  dh_next (from the FUTURE!)\n",
    "            dh = self.W_hy.T @ do + dh_next\n",
    "\n",
    "            # --- Through tanh activation ---\n",
    "            # dL/dz_t = dL/dh_t * tanh'(z_t)\n",
    "            dz = dh * tanh_derivative(zs[t])\n",
    "\n",
    "            # --- Gradients for W_xh, W_hh, b_h ---\n",
    "            dW_xh += dz @ xs[t].T     # input contribution\n",
    "            dW_hh += dz @ hs[t-1].T   # recurrent contribution\n",
    "            db_h  += dz\n",
    "\n",
    "            # --- Pass gradient to previous time step ---\n",
    "            # This is the RECURRENT gradient: dL/dh_{t-1} = W_hh^T · dz\n",
    "            # ⚠️ If W_hh has small eigenvalues, this shrinks → VANISHING GRADIENT\n",
    "            # ⚠️ If W_hh has large eigenvalues, this grows  → EXPLODING GRADIENT\n",
    "            dh_next = self.W_hh.T @ dz\n",
    "\n",
    "        # --- Gradient clipping (prevent exploding gradients) ---\n",
    "        for grad in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "        return dW_xh, dW_hh, dW_hy, db_h, db_y\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2c. PARAMETER UPDATE (Adagrad)\n",
    "    # -----------------------------------------------------------------\n",
    "    def update(self, grads):\n",
    "        \"\"\"\n",
    "        Adagrad update: lr_effective = lr / sqrt(sum_of_squared_grads)\n",
    "\n",
    "        This adaptively reduces the learning rate for frequently updated\n",
    "        parameters, which is helpful for RNNs where some characters\n",
    "        appear much more often than others.\n",
    "        \"\"\"\n",
    "        params = [self.W_xh,  self.W_hh,  self.W_hy,  self.b_h,  self.b_y]\n",
    "        mems   = [self.mW_xh, self.mW_hh, self.mW_hy, self.mb_h, self.mb_y]\n",
    "\n",
    "        for param, grad, mem in zip(params, grads, mems):\n",
    "            mem += grad * grad                              # accumulate squared gradient\n",
    "            param -= self.lr * grad / (np.sqrt(mem) + 1e-8) # adaptive update\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2d. SAMPLING (generate text from the model)\n",
    "    # -----------------------------------------------------------------\n",
    "    def sample(self, seed_idx, h, length):\n",
    "        \"\"\"\n",
    "        Generate a sequence of `length` tokens.\n",
    "\n",
    "        Uses the RNN autoregressively: feed the output back as the next input.\n",
    "\n",
    "        seed_idx : int — starting token index\n",
    "        h        : (hidden_size, 1) — starting hidden state\n",
    "        length   : int — number of tokens to generate\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.W_xh.shape[1], 1))\n",
    "        x[seed_idx] = 1.0\n",
    "        indices = []\n",
    "\n",
    "        for _ in range(length):\n",
    "            # Forward one step\n",
    "            h = tanh(self.W_xh @ x + self.W_hh @ h + self.b_h)\n",
    "            o = self.W_hy @ h + self.b_y\n",
    "            p = softmax(o)\n",
    "\n",
    "            # Sample from probability distribution\n",
    "            idx = np.random.choice(range(p.shape[0]), p=p.ravel())\n",
    "            indices.append(idx)\n",
    "\n",
    "            # Prepare next input\n",
    "            x = np.zeros_like(x)\n",
    "            x[idx] = 1.0\n",
    "\n",
    "        return indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. TRAINING LOOP — Character-level language model\n",
    "# =============================================================================\n",
    "\n",
    "def train_char_rnn():\n",
    "    \"\"\"\n",
    "    Train the RNN on a small text to learn character-level patterns.\n",
    "\n",
    "    Data flow overview:\n",
    "    ┌────────────────────────────────────────────────────────┐\n",
    "    │  \"hello\" → ['h','e','l','l','o']                       │\n",
    "    │                                                        │\n",
    "    │  Input:   h → e → l → l       (characters 0 to T-1)   │\n",
    "    │  Target:  e → l → l → o       (characters 1 to T)     │\n",
    "    │                                                        │\n",
    "    │  The RNN learns: given this character, predict the next │\n",
    "    └────────────────────────────────────────────────────────┘\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Dataset ---\n",
    "    text = \"hello world. hello neural network. hello recurrent neural network. \"\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    # Character ↔ Index mappings\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "    print(f\"Vocabulary ({vocab_size} chars): {chars}\")\n",
    "    print(f\"Text length: {len(text)} characters\\n\")\n",
    "\n",
    "    # --- Model ---\n",
    "    hidden_size = 64*2\n",
    "    seq_length = 25  # number of steps to unroll for BPTT\n",
    "    rnn = VanillaRNN(vocab_size, hidden_size, vocab_size, lr=0.01)\n",
    "\n",
    "    # --- Training ---\n",
    "    num_iterations = 10001\n",
    "    pointer = 0          # position in the text\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "    smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # initial loss estimate\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "\n",
    "        # --- Reset if we've reached end of text ---\n",
    "        if pointer + seq_length + 1 >= len(text):\n",
    "            pointer = 0\n",
    "            h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # --- Prepare input/target sequences ---\n",
    "        inputs  = [char_to_idx[ch] for ch in text[pointer:pointer + seq_length]]\n",
    "        targets = [char_to_idx[ch] for ch in text[pointer + 1:pointer + seq_length + 1]]\n",
    "\n",
    "        # --- Forward ---\n",
    "        loss, cache = rnn.forward(inputs, targets, h_prev)\n",
    "\n",
    "        # --- Backward ---\n",
    "        grads = rnn.backward(inputs, targets, cache)\n",
    "\n",
    "        # --- Update ---\n",
    "        rnn.update(grads)\n",
    "\n",
    "        # --- Carry hidden state forward (truncated BPTT) ---\n",
    "        # We keep h from the end of this chunk as the start of the next.\n",
    "        # This gives the RNN some \"memory\" across chunks, but we don't\n",
    "        # backpropagate across chunk boundaries (that's the \"truncated\" part).\n",
    "        h_prev = cache['hs'][len(inputs) - 1]\n",
    "\n",
    "        # --- Logging ---\n",
    "        smooth_loss = 0.999 * smooth_loss + 0.001 * loss\n",
    "        if iteration % 500 == 0:\n",
    "            print(f\"\\n--- Iteration {iteration}, Loss: {smooth_loss:.4f} ---\")\n",
    "\n",
    "            # Sample from the model\n",
    "            sample_indices = rnn.sample(inputs[0], h_prev, 60)\n",
    "            sample_text = ''.join([idx_to_char[i] for i in sample_indices])\n",
    "            print(f\"Sample: {sample_text}\")\n",
    "\n",
    "        pointer += seq_length\n",
    "\n",
    "    # --- Final visualization ---\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"WHAT THE WEIGHTS LEARNED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nW_xh shape: {rnn.W_xh.shape}  — Maps {vocab_size} input chars → {hidden_size} hidden units\")\n",
    "    print(f\"W_hh shape: {rnn.W_hh.shape}  — Recurrence: {hidden_size} hidden → {hidden_size} hidden\")\n",
    "    print(f\"W_hy shape: {rnn.W_hy.shape}  — Maps {hidden_size} hidden units → {vocab_size} output chars\")\n",
    "    print(f\"\\nTotal parameters: {sum(p.size for p in [rnn.W_xh, rnn.W_hh, rnn.W_hy, rnn.b_h, rnn.b_y])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e181076",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab011463",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Predicting Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ac09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== AIRLINE PASSENGERS (1949-1960) =====\n",
    "airline_data = {\n",
    "    'name': 'Airline Passengers (monthly, 1949-1960)',\n",
    "    'values': np.array([\n",
    "        112,118,132,129,121,135,148,148,136,119,104,118,\n",
    "        115,126,141,135,125,149,170,170,158,133,114,140,\n",
    "        145,150,178,163,172,178,199,199,184,162,146,166,\n",
    "        171,180,193,181,183,218,230,242,209,191,172,194,\n",
    "        196,196,236,235,229,243,264,272,237,211,180,201,\n",
    "        204,188,235,227,234,264,302,293,259,229,203,229,\n",
    "        242,233,267,269,270,315,364,347,312,274,237,278,\n",
    "        284,277,317,313,318,374,413,405,355,306,271,306,\n",
    "        315,301,356,348,355,422,465,467,404,347,305,336,\n",
    "        340,318,362,348,363,435,491,505,404,359,310,337,\n",
    "        360,342,406,396,420,472,548,559,463,407,362,405,\n",
    "        417,391,419,461,472,535,622,606,508,461,390,432\n",
    "    ], dtype=float),\n",
    "    'xlabel': 'Month',\n",
    "    'ylabel': 'Passengers (thousands)'\n",
    "}\n",
    "\n",
    "# ===== GLOBAL TEMPERATURE ANOMALY (1880-2023) =====\n",
    "temp_data = {\n",
    "    'name': 'Global Temperature Anomaly (annual, 1880-2023)',\n",
    "    'values': np.array([\n",
    "        -0.16,-0.08,-0.10,-0.17,-0.28,-0.32,-0.31,-0.36,-0.17,-0.10,\n",
    "        -0.35,-0.25,-0.30,-0.32,-0.32,-0.23,-0.11,-0.11,-0.27,-0.17,\n",
    "        -0.09,-0.16,-0.28,-0.37,-0.47,-0.29,-0.22,-0.39,-0.43,-0.48,\n",
    "        -0.43,-0.44,-0.36,-0.34,-0.15,-0.14,-0.36,-0.46,-0.29,-0.47,\n",
    "        -0.43,-0.44,-0.23,-0.23,-0.22,-0.28,-0.13,-0.21,-0.36,-0.23,\n",
    "        -0.27,-0.32,-0.29,-0.31,-0.12,-0.15,-0.16,-0.33,-0.43,-0.37,\n",
    "        -0.35,-0.24,-0.28,-0.27,-0.15,-0.09,-0.18,-0.20,-0.22,-0.25,\n",
    "        -0.11,-0.06,-0.09,-0.14,-0.12,-0.07,-0.01,-0.04,0.05,-0.07,\n",
    "        -0.01,0.08,0.04,0.07,0.12,-0.02,0.06,-0.02,0.06,0.03,\n",
    "        -0.03,-0.07,-0.02,0.01,0.13,-0.14,-0.07,-0.01,-0.05,0.05,\n",
    "        0.03,0.06,0.02,-0.03,-0.20,0.04,0.06,-0.07,0.03,0.02,\n",
    "        -0.08,0.05,0.03,0.23,0.08,0.10,0.15,0.12,0.17,0.32,\n",
    "        0.14,0.17,0.25,0.23,0.15,0.21,0.38,0.32,0.36,0.24,\n",
    "        0.28,0.32,0.41,0.32,0.46,0.42,0.55,0.62,0.63,0.54,\n",
    "        0.62,0.64,0.56,0.65\n",
    "    ], dtype=float),\n",
    "    'xlabel': 'Year (1880-2023)',\n",
    "    'ylabel': 'Temp Anomaly (°C)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c017e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets\n",
    "ALL_DATASETS = {\n",
    "    'airline': airline_data,\n",
    "    'temperature': temp_data,\n",
    "}\n",
    "\n",
    "# Quick preview\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5.5))\n",
    "for ax, (key, ds) in zip(axes, ALL_DATASETS.items()):\n",
    "    ax.plot(ds['values'], color='#38bdf8', linewidth=1.2)\n",
    "    ax.set_title(ds['name'], fontsize=8, pad=6)\n",
    "    ax.set_xlabel(ds['xlabel'], fontsize=7)\n",
    "fig.suptitle('Available Datasets', fontsize=11, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    \"\"\"\n",
    "    Vanilla RNN for time series prediction.\n",
    "    Input:  scalar x_t (one value per time step)\n",
    "    Output: scalar ŷ_t (predicted next value)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, lr=0.005):\n",
    "        self.hs = hidden_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Xavier initialization\n",
    "        s = lambda n: 1.0 / np.sqrt(n)\n",
    "        self.W_xh = np.random.randn(hidden_size, 1) * s(1)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * s(hidden_size)\n",
    "        self.W_hy = np.random.randn(1, hidden_size) * s(hidden_size)\n",
    "        self.b_h  = np.zeros((hidden_size, 1))\n",
    "        self.b_y  = np.zeros((1, 1))\n",
    "        \n",
    "        # Adagrad memory\n",
    "        self.mW_xh = np.zeros_like(self.W_xh)\n",
    "        self.mW_hh = np.zeros_like(self.W_hh)\n",
    "        self.mW_hy = np.zeros_like(self.W_hy)\n",
    "        self.mb_h  = np.zeros_like(self.b_h)\n",
    "        self.mb_y  = np.zeros_like(self.b_y)\n",
    "    \n",
    "    def forward(self, inputs, targets, h_prev):\n",
    "        \"\"\"Forward pass through the sequence.\"\"\"\n",
    "        T = len(inputs)\n",
    "        xs, hs, zs, ys = {}, {}, {}, {}\n",
    "        hs[-1] = h_prev.copy()\n",
    "        loss = 0.0\n",
    "        \n",
    "        for t in range(T):\n",
    "            xs[t] = np.array([[inputs[t]]])\n",
    "            zs[t] = self.W_xh @ xs[t] + self.W_hh @ hs[t-1] + self.b_h\n",
    "            hs[t] = np.tanh(zs[t])\n",
    "            ys[t] = self.W_hy @ hs[t] + self.b_y\n",
    "            err = ys[t][0,0] - targets[t]\n",
    "            loss += 0.5 * err * err\n",
    "        \n",
    "        return loss, {'xs': xs, 'hs': hs, 'zs': zs, 'ys': ys}\n",
    "    \n",
    "    def backward(self, inputs, targets, cache):\n",
    "        \"\"\"BPTT — Backpropagation Through Time.\"\"\"\n",
    "        xs, hs, zs, ys = cache['xs'], cache['hs'], cache['zs'], cache['ys']\n",
    "        T = len(inputs)\n",
    "        \n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h  = np.zeros_like(self.b_h)\n",
    "        db_y  = np.zeros_like(self.b_y)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            # MSE gradient: dL/dy = y - target\n",
    "            dy = np.array([[ys[t][0,0] - targets[t]]])\n",
    "            \n",
    "            dW_hy += dy @ hs[t].T\n",
    "            db_y  += dy\n",
    "            \n",
    "            dh = self.W_hy.T @ dy + dh_next\n",
    "            dz = dh * (1 - np.tanh(zs[t])**2)\n",
    "            \n",
    "            dW_xh += dz @ xs[t].T\n",
    "            dW_hh += dz @ hs[t-1].T\n",
    "            db_h  += dz\n",
    "            \n",
    "            dh_next = self.W_hh.T @ dz\n",
    "        \n",
    "        # Clip gradients\n",
    "        for g in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(g, -5, 5, out=g)\n",
    "        \n",
    "        return dW_xh, dW_hh, dW_hy, db_h, db_y\n",
    "    \n",
    "    def update(self, grads):\n",
    "        \"\"\"Adagrad update.\"\"\"\n",
    "        params = [self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y]\n",
    "        mems   = [self.mW_xh, self.mW_hh, self.mW_hy, self.mb_h, self.mb_y]\n",
    "        for p, g, m in zip(params, grads, mems):\n",
    "            m += g * g\n",
    "            p -= self.lr * g / (np.sqrt(m) + 1e-8)\n",
    "    \n",
    "    def predict(self, inputs, h_prev):\n",
    "        \"\"\"Run forward pass and return predictions + hidden states.\"\"\"\n",
    "        h = h_prev.copy()\n",
    "        predictions = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        for x in inputs:\n",
    "            z = self.W_xh @ np.array([[x]]) + self.W_hh @ h + self.b_h\n",
    "            h = np.tanh(z)\n",
    "            y = self.W_hy @ h + self.b_y\n",
    "            predictions.append(y[0,0])\n",
    "            hidden_states.append(h.flatten().copy())\n",
    "        \n",
    "        return np.array(predictions), np.array(hidden_states), h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET     = 'airline'    # 'airline', 'sunspots', 'temperature', 'co2', 'ecg'\n",
    "HIDDEN_SIZE = 32           # 8, 16, 32, 64 — more = more capacity\n",
    "SEQ_LENGTH  = 40           # 10-60 — how many steps to unroll BPTT\n",
    "LR          = 0.05        # 0.001-0.02\n",
    "EPOCHS      = 500          # total training epochs\n",
    "TRAIN_SPLIT = 0.8          # 80% train, 20% test\n",
    "PLOT_EVERY  = 10           # update the live plot every N epochs\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Load and normalize\n",
    "ds = ALL_DATASETS[DATASET]\n",
    "raw = ds['values']\n",
    "v_min, v_max = raw.min(), raw.max()\n",
    "data = (raw - v_min) / (v_max - v_min)  # normalize to [0, 1]\n",
    "\n",
    "split = int(len(data) * TRAIN_SPLIT)\n",
    "train_data = data[:split]\n",
    "test_data  = data[split:]\n",
    "\n",
    "def denorm(v):\n",
    "    return v * (v_max - v_min) + v_min\n",
    "\n",
    "print(f'Dataset:     {ds[\"name\"]}')\n",
    "print(f'Data points: {len(data)} (train: {len(train_data)}, test: {len(test_data)})')\n",
    "print(f'Hidden size: {HIDDEN_SIZE}')\n",
    "print(f'Seq length:  {SEQ_LENGTH}')\n",
    "print(f'LR:          {LR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "rnn = VanillaRNN(HIDDEN_SIZE, lr=LR)\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(f'Training on: {ds[\"name\"]}')\n",
    "print(f'Parameters: {HIDDEN_SIZE + HIDDEN_SIZE**2 + HIDDEN_SIZE + HIDDEN_SIZE + 1:,}')\n",
    "print(f'Training for {EPOCHS} epochs, plotting every {PLOT_EVERY}...\\n')\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(3, 2, figure=fig, hspace=0.35, wspace=0.25)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    # === TRAIN ONE EPOCH ===\n",
    "    h = np.zeros((HIDDEN_SIZE, 1))\n",
    "    epoch_loss = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for p in range(0, len(train_data) - SEQ_LENGTH - 1, SEQ_LENGTH):\n",
    "        inputs  = train_data[p : p + SEQ_LENGTH]\n",
    "        targets = train_data[p+1 : p + SEQ_LENGTH + 1]\n",
    "        \n",
    "        loss, cache = rnn.forward(inputs, targets, h)\n",
    "        grads = rnn.backward(inputs, targets, cache)\n",
    "        rnn.update(grads)\n",
    "        \n",
    "        h = cache['hs'][SEQ_LENGTH - 1]\n",
    "        epoch_loss += loss\n",
    "        steps += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / (steps * SEQ_LENGTH) if steps > 0 else 0\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Test loss\n",
    "    if len(test_data) > SEQ_LENGTH + 1:\n",
    "        h_test = np.zeros((HIDDEN_SIZE, 1))\n",
    "        # Feed train data to build hidden state\n",
    "        _, _, h_test = rnn.predict(train_data, h_test)\n",
    "        t_loss = 0\n",
    "        t_steps = 0\n",
    "        for p in range(0, len(test_data) - SEQ_LENGTH - 1, SEQ_LENGTH):\n",
    "            inp = test_data[p : p + SEQ_LENGTH]\n",
    "            tgt = test_data[p+1 : p + SEQ_LENGTH + 1]\n",
    "            l, _ = rnn.forward(inp, tgt, h_test)\n",
    "            t_loss += l\n",
    "            t_steps += 1\n",
    "        test_loss_history.append(t_loss / (t_steps * SEQ_LENGTH) if t_steps > 0 else 0)\n",
    "    \n",
    "    # === LIVE PLOT ===\n",
    "    if epoch % PLOT_EVERY == 0 or epoch == 1 or epoch == EPOCHS:\n",
    "        clear_output(wait=True)\n",
    "        fig.clear()\n",
    "        \n",
    "        # Get full predictions\n",
    "        h0 = np.zeros((HIDDEN_SIZE, 1))\n",
    "        all_preds, all_hidden, _ = rnn.predict(data[:-1], h0)\n",
    "        true_vals = data[1:]\n",
    "        \n",
    "        # --- Plot 1: Prediction vs Truth (FULL WIDTH) ---\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        x_axis = np.arange(len(true_vals))\n",
    "        \n",
    "        # Train/test background\n",
    "        ax1.axvspan(0, split-1, alpha=0.05, color='#38bdf8')\n",
    "        ax1.axvspan(split-1, len(true_vals), alpha=0.05, color='#f97316')\n",
    "        ax1.axvline(x=split-1, color='#ffffff', alpha=0.15, linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Fill error area\n",
    "        preds_clipped = np.clip(all_preds, 0, 1)\n",
    "        ax1.fill_between(x_axis, denorm(true_vals), denorm(preds_clipped),\n",
    "                         alpha=0.1, color='#ef4444')\n",
    "        \n",
    "        ax1.plot(x_axis, denorm(true_vals), color='#38bdf8', linewidth=1.5,\n",
    "                 label='Actual', alpha=0.9)\n",
    "        ax1.plot(x_axis, denorm(preds_clipped), color='#f97316', linewidth=2,\n",
    "                 label='RNN Prediction', alpha=0.9)\n",
    "        \n",
    "        # Labels\n",
    "        ax1.text(split*0.4, ax1.get_ylim()[1]*0.95, 'TRAIN',\n",
    "                 color='#38bdf8', alpha=0.4, fontsize=10, ha='center')\n",
    "        ax1.text(split + (len(true_vals)-split)*0.5, ax1.get_ylim()[1]*0.95, 'TEST',\n",
    "                 color='#f97316', alpha=0.4, fontsize=10, ha='center')\n",
    "        \n",
    "        ax1.set_title(f'{ds[\"name\"]}  —  Epoch {epoch}/{EPOCHS}  |  Loss: {avg_loss:.6f}',\n",
    "                      fontsize=11, pad=8)\n",
    "        ax1.set_ylabel(ds['ylabel'], fontsize=8)\n",
    "        ax1.legend(loc='upper left', fontsize=8)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # --- Plot 2: Loss Curve ---\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        ax2.plot(loss_history, color='#f97316', linewidth=1.5, label='Train Loss')\n",
    "        if test_loss_history:\n",
    "            ax2.plot(test_loss_history, color='#38bdf8', linewidth=1.5,\n",
    "                     label='Test Loss', alpha=0.7)\n",
    "        ax2.fill_between(range(len(loss_history)), loss_history,\n",
    "                         alpha=0.08, color='#f97316')\n",
    "        ax2.set_title('Loss Curve', fontsize=10, pad=6)\n",
    "        ax2.set_xlabel('Epoch', fontsize=8)\n",
    "        ax2.set_ylabel('MSE Loss', fontsize=8)\n",
    "        ax2.legend(fontsize=7)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # --- Plot 3: Hidden State Heatmap ---\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        # Show last 100 time steps\n",
    "        n_show = min(100, all_hidden.shape[0])\n",
    "        heatmap_data = all_hidden[-n_show:].T  # (hidden_size, time)\n",
    "        im = ax3.imshow(heatmap_data, aspect='auto', cmap='RdBu_r',\n",
    "                        vmin=-1, vmax=1, interpolation='nearest')\n",
    "        ax3.set_title('Hidden State Activations (last 100 steps)', fontsize=10, pad=6)\n",
    "        ax3.set_xlabel('Time Step', fontsize=8)\n",
    "        ax3.set_ylabel('Hidden Unit', fontsize=8)\n",
    "        plt.colorbar(im, ax=ax3, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # --- Plot 4: Weight Histograms ---\n",
    "        ax4 = fig.add_subplot(gs[2, 0])\n",
    "        ax4.hist(rnn.W_xh.flatten(), bins=20, alpha=0.6, color='#f97316', label='W_xh')\n",
    "        ax4.hist(rnn.W_hh.flatten(), bins=20, alpha=0.6, color='#38bdf8', label='W_hh')\n",
    "        ax4.hist(rnn.W_hy.flatten(), bins=20, alpha=0.6, color='#22c55e', label='W_hy')\n",
    "        ax4.set_title('Weight Distributions', fontsize=10, pad=6)\n",
    "        ax4.legend(fontsize=7)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # --- Plot 5: Per-step Error ---\n",
    "        ax5 = fig.add_subplot(gs[2, 1])\n",
    "        errors = np.abs(denorm(true_vals) - denorm(preds_clipped))\n",
    "        ax5.bar(x_axis[:split-1], errors[:split-1], color='#38bdf8', alpha=0.5, width=1.0, label='Train')\n",
    "        ax5.bar(x_axis[split-1:], errors[split-1:], color='#f97316', alpha=0.5, width=1.0, label='Test')\n",
    "        ax5.set_title('Absolute Error Per Step', fontsize=10, pad=6)\n",
    "        ax5.set_ylabel('|Error|', fontsize=8)\n",
    "        ax5.legend(fontsize=7)\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        \n",
    "print(f'\\n✅ Done! Final train loss: {loss_history[-1]:.6f}')\n",
    "if test_loss_history:\n",
    "    print(f'   Final test loss:  {test_loss_history[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2f7979",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ALL_DATASETS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m PLOT_EVERY  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m           \u001b[38;5;66;03m# update the live plot every N epochs\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load and normalize\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mALL_DATASETS\u001b[49m[DATASET]\n\u001b[1;32m     13\u001b[0m raw \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m v_min, v_max \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mmin(), raw\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ALL_DATASETS' is not defined"
     ]
    }
   ],
   "source": [
    "DATASET     = 'temperature'    # 'airline', 'sunspots', 'temperature', 'co2', 'ecg'\n",
    "HIDDEN_SIZE = 32*2           # 8, 16, 32, 64 — more = more capacity\n",
    "SEQ_LENGTH  = 40           # 10-60 — how many steps to unroll BPTT\n",
    "LR          = 0.001        # 0.001-0.02\n",
    "EPOCHS      = 10          # total training epochs\n",
    "TRAIN_SPLIT = 0.8          # 80% train, 20% test\n",
    "PLOT_EVERY  = 10           # update the live plot every N epochs\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Load and normalize\n",
    "ds = ALL_DATASETS[DATASET]\n",
    "raw = ds['values']\n",
    "v_min, v_max = raw.min(), raw.max()\n",
    "data = (raw - v_min) / (v_max - v_min)  # normalize to [0, 1]\n",
    "\n",
    "split = int(len(data) * TRAIN_SPLIT)\n",
    "train_data = data[:split]\n",
    "test_data  = data[split:]\n",
    "\n",
    "def denorm(v):\n",
    "    return v * (v_max - v_min) + v_min\n",
    "\n",
    "print(f'Dataset:     {ds[\"name\"]}')\n",
    "print(f'Data points: {len(data)} (train: {len(train_data)}, test: {len(test_data)})')\n",
    "print(f'Hidden size: {HIDDEN_SIZE}')\n",
    "print(f'Seq length:  {SEQ_LENGTH}')\n",
    "print(f'LR:          {LR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "rnn = VanillaRNN(HIDDEN_SIZE, lr=LR)\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print(f'Training on: {ds[\"name\"]}')\n",
    "print(f'Parameters: {HIDDEN_SIZE + HIDDEN_SIZE**2 + HIDDEN_SIZE + HIDDEN_SIZE + 1:,}')\n",
    "print(f'Training for {EPOCHS} epochs, plotting every {PLOT_EVERY}...\\n')\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(3, 2, figure=fig, hspace=0.35, wspace=0.25)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    # === TRAIN ONE EPOCH ===\n",
    "    h = np.zeros((HIDDEN_SIZE, 1))\n",
    "    epoch_loss = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for p in range(0, len(train_data) - SEQ_LENGTH - 1, SEQ_LENGTH):\n",
    "        inputs  = train_data[p : p + SEQ_LENGTH]\n",
    "        targets = train_data[p+1 : p + SEQ_LENGTH + 1]\n",
    "        \n",
    "        loss, cache = rnn.forward(inputs, targets, h)\n",
    "        grads = rnn.backward(inputs, targets, cache)\n",
    "        rnn.update(grads)\n",
    "        \n",
    "        h = cache['hs'][SEQ_LENGTH - 1]\n",
    "        epoch_loss += loss\n",
    "        steps += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / (steps * SEQ_LENGTH) if steps > 0 else 0\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Test loss\n",
    "    if len(test_data) > SEQ_LENGTH + 1:\n",
    "        h_test = np.zeros((HIDDEN_SIZE, 1))\n",
    "        # Feed train data to build hidden state\n",
    "        _, _, h_test = rnn.predict(train_data, h_test)\n",
    "        t_loss = 0\n",
    "        t_steps = 0\n",
    "        for p in range(0, len(test_data) - SEQ_LENGTH - 1, SEQ_LENGTH):\n",
    "            inp = test_data[p : p + SEQ_LENGTH]\n",
    "            tgt = test_data[p+1 : p + SEQ_LENGTH + 1]\n",
    "            l, _ = rnn.forward(inp, tgt, h_test)\n",
    "            t_loss += l\n",
    "            t_steps += 1\n",
    "        test_loss_history.append(t_loss / (t_steps * SEQ_LENGTH) if t_steps > 0 else 0)\n",
    "    \n",
    "    # === LIVE PLOT ===\n",
    "    if epoch % PLOT_EVERY == 0 or epoch == 1 or epoch == EPOCHS:\n",
    "        clear_output(wait=True)\n",
    "        fig.clear()\n",
    "        \n",
    "        # Get full predictions\n",
    "        h0 = np.zeros((HIDDEN_SIZE, 1))\n",
    "        all_preds, all_hidden, _ = rnn.predict(data[:-1], h0)\n",
    "        true_vals = data[1:]\n",
    "        \n",
    "        # --- Plot 1: Prediction vs Truth (FULL WIDTH) ---\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        x_axis = np.arange(len(true_vals))\n",
    "        \n",
    "        # Train/test background\n",
    "        ax1.axvspan(0, split-1, alpha=0.05, color='#38bdf8')\n",
    "        ax1.axvspan(split-1, len(true_vals), alpha=0.05, color='#f97316')\n",
    "        ax1.axvline(x=split-1, color='#ffffff', alpha=0.15, linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Fill error area\n",
    "        preds_clipped = np.clip(all_preds, 0, 1)\n",
    "        ax1.fill_between(x_axis, denorm(true_vals), denorm(preds_clipped),\n",
    "                         alpha=0.1, color='#ef4444')\n",
    "        \n",
    "        ax1.plot(x_axis, denorm(true_vals), color='#38bdf8', linewidth=1.5,\n",
    "                 label='Actual', alpha=0.9)\n",
    "        ax1.plot(x_axis, denorm(preds_clipped), color='#f97316', linewidth=2,\n",
    "                 label='RNN Prediction', alpha=0.9)\n",
    "        \n",
    "        # Labels\n",
    "        ax1.text(split*0.4, ax1.get_ylim()[1]*0.95, 'TRAIN',\n",
    "                 color='#38bdf8', alpha=0.4, fontsize=10, ha='center')\n",
    "        ax1.text(split + (len(true_vals)-split)*0.5, ax1.get_ylim()[1]*0.95, 'TEST',\n",
    "                 color='#f97316', alpha=0.4, fontsize=10, ha='center')\n",
    "        \n",
    "        ax1.set_title(f'{ds[\"name\"]}  —  Epoch {epoch}/{EPOCHS}  |  Loss: {avg_loss:.6f}',\n",
    "                      fontsize=11, pad=8)\n",
    "        ax1.set_ylabel(ds['ylabel'], fontsize=8)\n",
    "        ax1.legend(loc='upper left', fontsize=8)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # --- Plot 2: Loss Curve ---\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        ax2.plot(loss_history, color='#f97316', linewidth=1.5, label='Train Loss')\n",
    "        if test_loss_history:\n",
    "            ax2.plot(test_loss_history, color='#38bdf8', linewidth=1.5,\n",
    "                     label='Test Loss', alpha=0.7)\n",
    "        ax2.fill_between(range(len(loss_history)), loss_history,\n",
    "                         alpha=0.08, color='#f97316')\n",
    "        ax2.set_title('Loss Curve', fontsize=10, pad=6)\n",
    "        ax2.set_xlabel('Epoch', fontsize=8)\n",
    "        ax2.set_ylabel('MSE Loss', fontsize=8)\n",
    "        ax2.legend(fontsize=7)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # --- Plot 3: Hidden State Heatmap ---\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        # Show last 100 time steps\n",
    "        n_show = min(100, all_hidden.shape[0])\n",
    "        heatmap_data = all_hidden[-n_show:].T  # (hidden_size, time)\n",
    "        im = ax3.imshow(heatmap_data, aspect='auto', cmap='RdBu_r',\n",
    "                        vmin=-1, vmax=1, interpolation='nearest')\n",
    "        ax3.set_title('Hidden State Activations (last 100 steps)', fontsize=10, pad=6)\n",
    "        ax3.set_xlabel('Time Step', fontsize=8)\n",
    "        ax3.set_ylabel('Hidden Unit', fontsize=8)\n",
    "        plt.colorbar(im, ax=ax3, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # --- Plot 4: Weight Histograms ---\n",
    "        ax4 = fig.add_subplot(gs[2, 0])\n",
    "        ax4.hist(rnn.W_xh.flatten(), bins=20, alpha=0.6, color='#f97316', label='W_xh')\n",
    "        ax4.hist(rnn.W_hh.flatten(), bins=20, alpha=0.6, color='#38bdf8', label='W_hh')\n",
    "        ax4.hist(rnn.W_hy.flatten(), bins=20, alpha=0.6, color='#22c55e', label='W_hy')\n",
    "        ax4.set_title('Weight Distributions', fontsize=10, pad=6)\n",
    "        ax4.legend(fontsize=7)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # --- Plot 5: Per-step Error ---\n",
    "        ax5 = fig.add_subplot(gs[2, 1])\n",
    "        errors = np.abs(denorm(true_vals) - denorm(preds_clipped))\n",
    "        ax5.bar(x_axis[:split-1], errors[:split-1], color='#38bdf8', alpha=0.5, width=1.0, label='Train')\n",
    "        ax5.bar(x_axis[split-1:], errors[split-1:], color='#f97316', alpha=0.5, width=1.0, label='Test')\n",
    "        ax5.set_title('Absolute Error Per Step', fontsize=10, pad=6)\n",
    "        ax5.set_ylabel('|Error|', fontsize=8)\n",
    "        ax5.legend(fontsize=7)\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        \n",
    "print(f'\\n✅ Done! Final train loss: {loss_history[-1]:.6f}')\n",
    "if test_loss_history:\n",
    "    print(f'   Final test loss:  {test_loss_history[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2b5be",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa753267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegozapataalcaraz/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e0b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ev 0\n"
     ]
    }
   ],
   "source": [
    "# Database\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds[\"train\"][0][\"text\"][:200], ds[\"train\"][0][\"label\"])  # label: 0/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709b5040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds[\"train\"][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1988516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469a59b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]])\n",
      "['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "\n",
    "print(encoded_input[\"input_ids\"])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e4aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
       "          2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
       "          2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
       "          2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
       "          1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
       "          2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
       "          6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
       "          5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
       "         14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
       "          1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
       "          2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
       "         25430, 14728,  2245,  2055,  3056,  2576,  3314,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 128 \n",
    "tokens = tokenizer(example[\"text\"], truncation=True, padding='max_length', max_length=seq_length, return_tensors='pt')\n",
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46acc57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|███████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 361.10it/s, Materializing param=pooler.dense.weight]\n",
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad57c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5638b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "# Get last hidden states\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings.shape)  # (1, seq_length, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708af961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRN Layers\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296ca8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1, num_layers=1, bidirectional=False):\n",
    "        super(TrainableRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, \n",
    "                        num_layers=num_layers, \n",
    "                        batch_first=True,\n",
    "                        nonlinearity='tanh',\n",
    "                        dropout=dropout if num_layers > 1 else 0,\n",
    "                        bidirectional=bidirectional\n",
    "                        )\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * direction_factor, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape: (batch_size, seq_length, input_size)\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        # Take last time step\n",
    "        out = self.fc(rnn_out[:, -1, :])\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b735966",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = embeddings.shape[-1]  # hidden_size from BERT\n",
    "hidden_size = 128\n",
    "output_size = 2 # Binary classification\n",
    "\n",
    "model = TrainableRNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d39c055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit: tensor([[-0.4815,  0.5061]], grad_fn=<AddmmBackward0>)\n",
      "Probability: tensor([[0.3819, 0.6239]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(embeddings)\n",
    "prob = torch.sigmoid(logits)\n",
    "\n",
    "print(\"Logit:\", logits)\n",
    "print(\"Probability:\", prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88869a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6d9af14017793bd0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6d9af14017793bd0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing BERT embeddings (one time only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|███████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 466.39it/s, Materializing param=pooler.dense.weight]\n",
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0/25000\n",
      "  500/25000\n",
      "  1000/25000\n",
      "  1500/25000\n",
      "  2000/25000\n",
      "  2500/25000\n",
      "  3000/25000\n",
      "  3500/25000\n",
      "  4000/25000\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PRECOMPUTE BERT EMBEDDINGS (runs once, saves to disk)\n",
    "# =============================================================================\n",
    "\n",
    "seq_length = 128\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if os.path.exists('data/imdb_embeddings.pt'):\n",
    "    print(\"Loading precomputed embeddings...\")\n",
    "    all_embeddings = torch.load('data/imdb_embeddings.pt')\n",
    "    all_labels = torch.load('data/imdb_labels.pt')\n",
    "else:\n",
    "    print(\"Precomputing BERT embeddings (one time only)...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, text in enumerate(ds[\"train\"][\"text\"]):\n",
    "            tokens = tokenizer(text, truncation=True, padding='max_length',\n",
    "                             max_length=seq_length, return_tensors='pt').to(device)\n",
    "            emb = bert_model(**tokens).last_hidden_state.squeeze(0).cpu()\n",
    "            all_embeddings.append(emb)\n",
    "            if i % 500 == 0:\n",
    "                print(f\"  {i}/{len(ds['train']['text'])}\")\n",
    "\n",
    "    all_embeddings = torch.stack(all_embeddings)\n",
    "    all_labels = torch.tensor(ds[\"train\"][\"label\"], dtype=torch.float)\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    torch.save(all_embeddings, 'data/imdb_embeddings.pt')\n",
    "    torch.save(all_labels, 'data/imdb_labels.pt')\n",
    "    print(\"Saved to data/\")\n",
    "\n",
    "    del bert_model, tokenizer  # free GPU memory\n",
    "\n",
    "print(f\"Embeddings: {all_embeddings.shape}\")  # (25000, 128, 768)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. RNN CLASSIFIER\n",
    "# =============================================================================\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size,\n",
    "                         batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_seq, h_last = self.rnn(x)\n",
    "        out = h_last.squeeze(0)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. SETUP\n",
    "# =============================================================================\n",
    "\n",
    "train_dataset = TensorDataset(all_embeddings, all_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "rnn_model = SentimentRNN(input_size=768, hidden_size=128).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "num_epochs = 10\n",
    "writer = SummaryWriter('runs/rnn_imdb')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (embeddings, labels) in enumerate(train_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = rnn_model(embeddings)\n",
    "        loss = criterion(logits.squeeze(), labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        global_step = epoch * len(train_loader) + batch_idx\n",
    "        writer.add_scalar('Loss/batch', loss.item(), global_step)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/epoch', avg_loss, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save model\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "torch.save(rnn_model.state_dict(), 'checkpoints/rnn_imdb.pt')\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd78be4-3650-4805-b175-703c0980e2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
