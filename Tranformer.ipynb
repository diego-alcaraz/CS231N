{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b50d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f54b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea023085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Config\n",
    "# =========================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # data\n",
    "    data_path: str = \"tinyshakespeare.txt\"\n",
    "    url: str = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "    # training\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size: int = 64\n",
    "    block_size: int = 128          # context length (N)\n",
    "    max_iters: int = 3000\n",
    "    eval_every: int = 300\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # model\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 4\n",
    "    d_model: int = 256             # D\n",
    "    d_ff: int = 4 * 256            # MLP hidden\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Get dataset\n",
    "# =========================\n",
    "def ensure_data(cfg: CFG):\n",
    "    if os.path.exists(cfg.data_path):\n",
    "        return\n",
    "    print(f\"Downloading dataset to {cfg.data_path} ...\")\n",
    "    urllib.request.urlretrieve(cfg.url, cfg.data_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Char tokenizer\n",
    "# =========================\n",
    "class CharVocab:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[i] for i in ids)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Data batching\n",
    "# =========================\n",
    "def make_splits(data_ids: torch.Tensor, train_frac=0.9):\n",
    "    n = int(len(data_ids) * train_frac)\n",
    "    return data_ids[:n], data_ids[n:]\n",
    "\n",
    "\n",
    "def get_batch(data_ids: torch.Tensor, cfg: CFG):\n",
    "    # sample random positions\n",
    "    ix = torch.randint(0, len(data_ids) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "    x = torch.stack([data_ids[i:i + cfg.block_size] for i in ix])\n",
    "    y = torch.stack([data_ids[i + 1:i + cfg.block_size + 1] for i in ix])\n",
    "    return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_ids, val_ids, cfg: CFG, iters=50):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, ids in [(\"train\", train_ids), (\"val\", val_ids)]:\n",
    "        losses = []\n",
    "        for _ in range(iters):\n",
    "            x, y = get_batch(ids, cfg)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Transformer building blocks (from scratch)\n",
    "# =========================\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        # combined QKV projection (one matmul)\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # causal mask (precomputed)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)  # not a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, D)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)  # (B, N, 3D)\n",
    "        q, k, v = qkv.split(D, dim=2)  # each (B, N, D)\n",
    "\n",
    "        # reshape to heads\n",
    "        q = q.view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, Dh)\n",
    "        k = k.view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, Dh)\n",
    "        v = v.view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, Dh)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, H, N, N)\n",
    "\n",
    "        # causal mask: prevent attending to future tokens\n",
    "        att = att.masked_fill(self.mask[:, :, :N, :N] == 0, float(\"-inf\"))\n",
    "\n",
    "        w = F.softmax(att, dim=-1)              # (B, H, N, N)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        y = w @ v                               # (B, H, N, Dh)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, N, D)  # concat heads -> (B, N, D)\n",
    "\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN Transformer block:\n",
    "    x = x + Attn(LN(x))\n",
    "    x = x + MLP(LN(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, block_size, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = cfg.block_size\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "        self.drop = nn.Dropout(cfg.dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg.d_model, cfg.n_heads, cfg.block_size, cfg.d_ff, cfg.dropout)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, N)\n",
    "        B, N = idx.shape\n",
    "        if N > self.block_size:\n",
    "            raise ValueError(f\"Sequence length {N} exceeds block_size {self.block_size}\")\n",
    "\n",
    "        pos = torch.arange(0, N, device=idx.device).unsqueeze(0)  # (1, N)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)                 # (B, N, D)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, N, vocab)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0):\n",
    "        # idx: (B, N)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)              # (B, N, vocab)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Train\n",
    "# =========================\n",
    "def main():\n",
    "    torch.manual_seed(1337)\n",
    "\n",
    "    # data\n",
    "    ensure_data(cfg)\n",
    "    text = load_text(cfg.data_path)\n",
    "    vocab = CharVocab(text)\n",
    "    data = torch.tensor(vocab.encode(text), dtype=torch.long)\n",
    "\n",
    "    train_ids, val_ids = make_splits(data)\n",
    "\n",
    "    print(f\"Dataset chars: {len(text):,}\")\n",
    "    print(f\"Vocab size: {vocab.vocab_size}\")\n",
    "    print(f\"Train tokens: {len(train_ids):,} | Val tokens: {len(val_ids):,}\")\n",
    "    print(f\"Device: {cfg.device}\")\n",
    "\n",
    "    # model\n",
    "    model = TinyGPT(vocab.vocab_size, cfg).to(cfg.device)\n",
    "    print(f\"Params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(1, cfg.max_iters + 1):\n",
    "        x, y = get_batch(train_ids, cfg)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % cfg.eval_every == 0 or it == 1:\n",
    "            losses = estimate_loss(model, train_ids, val_ids, cfg, iters=30)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"iter {it:5d} | train loss {losses['train']:.3f} | val loss {losses['val']:.3f} | time {dt:.1f}s\")\n",
    "\n",
    "            # sample generation\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device=cfg.device)  # start token = arbitrary (0)\n",
    "            gen = model.generate(context, max_new_tokens=300, temperature=1.0)[0].tolist()\n",
    "            print(\"---- sample ----\")\n",
    "            print(vocab.decode(gen[:]))\n",
    "            print(\"--------------\\n\")\n",
    "\n",
    "    # final sample\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=cfg.device)\n",
    "    gen = model.generate(context, max_new_tokens=800, temperature=0.9)[0].tolist()\n",
    "    print(vocab.decode(gen))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
