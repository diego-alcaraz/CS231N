{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bda8a95",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b50d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979f54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Config\n",
    "# =========================\n",
    "\n",
    "class CFG:\n",
    "    # data\n",
    "    data_path: str = \"tinyshakespeare.txt\"\n",
    "    url: str = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "    # training\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size: int = 64\n",
    "    block_size: int = 128*2          # context length (N)\n",
    "    max_iters: int = 10000\n",
    "    eval_every: int = 1000\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # model\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 4\n",
    "    d_model: int = 256             # D\n",
    "    d_ff: int = 4 * 256            # MLP hidden\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b26359b-8f63-4c8d-8a90-252b0579e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Get dataset\n",
    "# =========================\n",
    "def ensure_data(cfg: CFG):\n",
    "    if os.path.exists(cfg.data_path):\n",
    "        return\n",
    "    print(f\"Downloading dataset to {cfg.data_path} ...\")\n",
    "    urllib.request.urlretrieve(cfg.url, cfg.data_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae015b0-6c09-4d3b-a05b-abbb60d2f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Char tokenizer\n",
    "# =========================\n",
    "class CharVocab:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[i] for i in ids)\n",
    "\n",
    "# ========================\n",
    "class CharVocab:\n",
    "    \"\"\"\n",
    "    Character-level tokenizer with special tokens and error handling.\n",
    "    \n",
    "    Usage:\n",
    "        vocab = CharVocab(text)\n",
    "        ids = vocab.encode(\"hello\")        # [5, 2, 9, 9, 12]\n",
    "        text = vocab.decode(ids)           # \"hello\"\n",
    "        ids = vocab.encode(\"xyz§\")         # unknown '§' → [UNK] token\n",
    "    \"\"\"\n",
    "\n",
    "    # Special tokens\n",
    "    PAD_TOKEN = \"<PAD>\"   # padding for batching\n",
    "    UNK_TOKEN = \"<UNK>\"   # unknown characters\n",
    "    BOS_TOKEN = \"<BOS>\"   # beginning of sequence\n",
    "    EOS_TOKEN = \"<EOS>\"   # end of sequence\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        special = [self.PAD_TOKEN, self.UNK_TOKEN, self.BOS_TOKEN, self.EOS_TOKEN]\n",
    "        chars = special + sorted(set(text))\n",
    "\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        # Quick access to special token IDs\n",
    "        self.pad_id = self.stoi[self.PAD_TOKEN]\n",
    "        self.unk_id = self.stoi[self.UNK_TOKEN]\n",
    "        self.bos_id = self.stoi[self.BOS_TOKEN]\n",
    "        self.eos_id = self.stoi[self.EOS_TOKEN]\n",
    "\n",
    "    def encode(self, s: str, add_special=False, max_length=None):\n",
    "        \"\"\"\n",
    "        Encode string → list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            s: input string\n",
    "            add_special: wrap with <BOS> and <EOS>\n",
    "            max_length: pad/truncate to fixed length\n",
    "\n",
    "        Examples:\n",
    "            vocab.encode(\"hi\")                          → [5, 6]\n",
    "            vocab.encode(\"hi\", add_special=True)        → [2, 5, 6, 3]\n",
    "            vocab.encode(\"hi\", max_length=5)            → [5, 6, 0, 0, 0]\n",
    "        \"\"\"\n",
    "        ids = [self.stoi.get(c, self.unk_id) for c in s]\n",
    "\n",
    "        if add_special:\n",
    "            ids = [self.bos_id] + ids + [self.eos_id]\n",
    "\n",
    "        if max_length is not None:\n",
    "            ids = ids[:max_length]                          # truncate\n",
    "            ids = ids + [self.pad_id] * (max_length - len(ids))  # pad\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids, strip_special=True):\n",
    "        \"\"\"\n",
    "        Decode list of token IDs → string.\n",
    "\n",
    "        Args:\n",
    "            ids: list of ints or tensor\n",
    "            strip_special: remove PAD/BOS/EOS from output\n",
    "        \"\"\"\n",
    "        # Handle tensors\n",
    "        if hasattr(ids, 'tolist'):\n",
    "            ids = ids.tolist()\n",
    "\n",
    "        special_ids = {self.pad_id, self.bos_id, self.eos_id}\n",
    "        chars = []\n",
    "        for i in ids:\n",
    "            if strip_special and i in special_ids:\n",
    "                continue\n",
    "            chars.append(self.itos.get(i, \"?\"))\n",
    "\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"CharVocab(size={self.vocab_size}, chars={''.join(self.itos[i] for i in range(4, min(24, self.vocab_size)))}...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a31d63-918d-4576-ba8c-3165cbf2baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Data batching\n",
    "# =========================\n",
    "def make_splits(data_ids: torch.Tensor, train_frac=0.9):\n",
    "    n = int(len(data_ids) * train_frac)\n",
    "    return data_ids[:n], data_ids[n:]\n",
    "\n",
    "\n",
    "def get_batch(data_ids: torch.Tensor, cfg: CFG):\n",
    "    # sample random positions\n",
    "    ix = torch.randint(0, len(data_ids) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "    x = torch.stack([data_ids[i:i + cfg.block_size] for i in ix])\n",
    "    y = torch.stack([data_ids[i + 1:i + cfg.block_size + 1] for i in ix])\n",
    "    return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_ids, val_ids, cfg: CFG, iters=50):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, ids in [(\"train\", train_ids), (\"val\", val_ids)]:\n",
    "        losses = []\n",
    "        for _ in range(iters):\n",
    "            x, y = get_batch(ids, cfg)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2ea47f-badc-456e-b2e4-f5c6dd2564a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Transformer building blocks (from scratch)\n",
    "# =========================\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        # combined QKV projection (one matmul)\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # causal mask (precomputed)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)  # not a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, D)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)  # (B, N, 3D)\n",
    "        q, k, v = qkv.split(D, dim=2)  # each (B, N, D)\n",
    "\n",
    "        # reshape to heads\n",
    "        q = q.view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, Dh)\n",
    "        k = k.view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, Dh)\n",
    "        v = v.view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, Dh)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, H, N, N)\n",
    "\n",
    "        # causal mask: prevent attending to future tokens\n",
    "        att = att.masked_fill(self.mask[:, :, :N, :N] == 0, float(\"-inf\"))\n",
    "\n",
    "        w = F.softmax(att, dim=-1)              # (B, H, N, N)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        y = w @ v                               # (B, H, N, Dh)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, N, D)  # concat heads -> (B, N, D)\n",
    "\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20dffc3e-b351-482e-85c5-1babc645ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN Transformer block:\n",
    "    x = x + Attn(LN(x))\n",
    "    x = x + MLP(LN(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, block_size, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = cfg.block_size\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "        self.drop = nn.Dropout(cfg.dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg.d_model, cfg.n_heads, cfg.block_size, cfg.d_ff, cfg.dropout)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, N)\n",
    "        B, N = idx.shape\n",
    "        if N > self.block_size:\n",
    "            raise ValueError(f\"Sequence length {N} exceeds block_size {self.block_size}\")\n",
    "\n",
    "        pos = torch.arange(0, N, device=idx.device).unsqueeze(0)  # (1, N)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)                 # (B, N, D)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, N, vocab)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0):\n",
    "        # idx: (B, N)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)              # (B, N, vocab)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea023085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chars: 1,115,394\n",
      "Vocab size: 69\n",
      "Train tokens: 1,003,854 | Val tokens: 111,540\n",
      "Device: cuda\n",
      "Params: 3.26M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(vocab\u001b[38;5;241m.\u001b[39mdecode(gen))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mmax_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     28\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m get_batch(train_ids, cfg)\n\u001b[0;32m---> 29\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 72\u001b[0m, in \u001b[0;36mTinyGPT.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     75\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)  \u001b[38;5;66;03m# (B, N, vocab)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1781\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1781\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Train\n",
    "# =========================\n",
    "def main():\n",
    "    torch.manual_seed(8)\n",
    "\n",
    "    # data\n",
    "    ensure_data(cfg)\n",
    "    text = load_text(cfg.data_path)\n",
    "    vocab = CharVocab(text)\n",
    "    data = torch.tensor(vocab.encode(text), dtype=torch.long)\n",
    "\n",
    "    train_ids, val_ids = make_splits(data)\n",
    "\n",
    "    print(f\"Dataset chars: {len(text):,}\")\n",
    "    print(f\"Vocab size: {vocab.vocab_size}\")\n",
    "    print(f\"Train tokens: {len(train_ids):,} | Val tokens: {len(val_ids):,}\")\n",
    "    print(f\"Device: {cfg.device}\")\n",
    "\n",
    "    # model\n",
    "    model = TinyGPT(vocab.vocab_size, cfg).to(cfg.device)\n",
    "    print(f\"Params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for it in range(1, cfg.max_iters + 1):\n",
    "        x, y = get_batch(train_ids, cfg)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % cfg.eval_every == 0:\n",
    "            losses = estimate_loss(model, train_ids, val_ids, cfg, iters=30)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"iter {it:5d} | train loss {losses['train']:.3f} | val loss {losses['val']:.3f} | time {dt:.1f}s\")\n",
    "\n",
    "            # sample generation\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device=cfg.device)  # start token = arbitrary (0)\n",
    "            gen = model.generate(context, max_new_tokens=300, temperature=1.0)[0].tolist()\n",
    "            print(\"---- sample ----\")\n",
    "            print(vocab.decode(gen[:]))\n",
    "            print(\"--------------\\n\")\n",
    "\n",
    "    # final sample\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=cfg.device)\n",
    "    gen = model.generate(context, max_new_tokens=800, temperature=0.9)[0].tolist()\n",
    "    print(vocab.decode(gen))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e74ac-952a-4693-8787-ec140cb45222",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5ad43-c082-4ca1-8b12-0998e336a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaad2d2-bb81-48fd-ad6b-1463359f3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Self-Attention core idea\n",
    "def simple_attention(query, key, value):\n",
    "    # T = sequence length\n",
    "    # D = Embedding dimension \n",
    "    # Step 1: How relevant is each key to my query ?\n",
    "    scores = query @ key.T # dim (T x T)\n",
    "\n",
    "    # Step 2: Scale down, prevents Softmax from saturing\n",
    "    d_k = key.shape[-1]\n",
    "    scores = scores/math.sqrt(d_k) #dim (T x T)\n",
    "\n",
    "    # Step 3: Normalise probabilities using Softmax\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 4: Output\n",
    "    output = weights @ value\n",
    "\n",
    "    return output, weights\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cee53f-ec0c-4a5b-bc00-fc137640bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4 tokens x 8 dims\n",
      "Output: torch.Size([4, 8])\n",
      "\n",
      "Attention weights\n",
      "  Rows = 'from' token, Cols = 'to' token\n",
      "            The    cat    sat   down\n",
      "   The →  0.321  0.451  0.149  0.079  (sum=1.000)\n",
      "   cat →  0.008  0.988  0.002  0.001  (sum=1.000)\n",
      "   sat →  0.138  0.101  0.605  0.156  (sum=1.000)\n",
      "  down →  0.065  0.063  0.137  0.735  (sum=1.000)\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "\n",
    "T, D = 4, 8\n",
    "x = torch.randn(T, D)\n",
    "\n",
    "# In self-attention, Q,K,V all come from the same input\n",
    "output, weights = simple_attention(x, x, x)\n",
    "\n",
    "print(f\"Input: {T} tokens x {D} dims\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "\n",
    "print(\"\\nAttention weights\")\n",
    "print(f\"  Rows = 'from' token, Cols = 'to' token\")\n",
    "\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "print(f\"         {' '.join(f'{t:>6}' for t in tokens)}\")\n",
    "for i, row in enumerate(weights):\n",
    "    vals = ' '.join(f'{v:>6.3f}' for v in row)\n",
    "    print(f\"  {tokens[i]:>4} → {vals}  (sum={row.sum():.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d9df5-c5d7-4199-b1ad-ed73730d7047",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd676088-b3ab-4578-b32e-19aab37bf4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # must be integer\n",
    "\n",
    "        # Linear Projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # Project (B, T, D) > (B, T, H, d_k)\n",
    "        # and reshape (B, T, H, d_k) > (B, H, T, d_k)\n",
    "        Q = self.W_Q(x).view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = self.W_K(x).view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = self.W_V(x).view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # Similarity score per head\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k) # (B, H, T, T)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1) \n",
    "        # weights: (B, H, T, T) and V: (B, H, T, d_k)\n",
    "        att_out = weights @ V # (B, H, T, d_k)\n",
    "\n",
    "        # Concatenate heads: (B, H, T, d_k) → (B, T, H*d_k) = (B, T, D)\n",
    "        att_out = att_out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "\n",
    "        output = self.W_O(att_out)\n",
    "        return output, weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b076a42-4829-4f26-aff2-58709d8ebb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:    (B=1, T=4, D=64)\n",
      "Heads:    8, each with d_k=8\n",
      "Weights:  torch.Size([1, 8, 4, 4]) → (batch, heads, from_token, to_token)\n",
      "Output:   torch.Size([1, 4, 64])\n",
      "\n",
      "Each head learns different relationships:\n",
      "  Head 0 weights (token→token):\n",
      "    token 0: [0.352 0.226 0.146 0.275]\n",
      "    token 1: [0.405 0.194 0.201 0.200]\n",
      "    token 2: [0.225 0.296 0.205 0.274]\n",
      "    token 3: [0.303 0.345 0.152 0.199]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B, T, D, H = 1, 4, 64, 8\n",
    "mha = MultiHeadAttention(D, H)\n",
    "x = torch.randn(B, T, D)\n",
    "\n",
    "output, weights = mha(x)\n",
    "print(f\"\\nInput:    (B={B}, T={T}, D={D})\")\n",
    "print(f\"Heads:    {H}, each with d_k={D // H}\")\n",
    "print(f\"Weights:  {weights.shape} → (batch, heads, from_token, to_token)\")\n",
    "print(f\"Output:   {output.shape}\")\n",
    "print(f\"\\nEach head learns different relationships:\")\n",
    "print(f\"  Head 0 weights (token→token):\")\n",
    "for i in range(T):\n",
    "    vals = ' '.join(f'{v:.3f}' for v in weights[0, 0, i])\n",
    "    print(f\"    token {i}: [{vals}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67681296-77c9-43db-a28d-654c3314513a",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e5e3e6-fc5b-409b-a6d1-8acdf470a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0)/ d_model)\n",
    "        )\n",
    "\n",
    "        # Even dim\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        # Odd dim\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0) #(1, max_len, D)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        # Add position encoding (same for every batch)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c70d1c-1694-47a2-a997-a95a7666a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Position signals (first 8 dims of each position):\n",
      "  pos 0: [ 0.000  1.000  0.000  1.000  0.000  1.000  0.000  1.000 ...]\n",
      "  pos 1: [ 0.841  0.540  0.311  0.950  0.100  0.995  0.032  1.000 ...]\n",
      "  pos 2: [ 0.909 -0.416  0.591  0.807  0.199  0.980  0.063  0.998 ...]\n",
      "  pos 3: [ 0.141 -0.990  0.813  0.583  0.296  0.955  0.095  0.996 ...]\n",
      "  pos 4: [-0.757 -0.654  0.954  0.301  0.389  0.921  0.126  0.992 ...]\n",
      "  pos 5: [-0.959  0.284  1.000 -0.010  0.479  0.878  0.157  0.988 ...]\n",
      "\n",
      "Notice: each position has a unique pattern.\n",
      "Lower dims change fast (high freq), higher dims change slow (low freq).\n"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoding(d_model=16, max_len=100)\n",
    "x = torch.zeros(1, 6, 16)  # 6 positions\n",
    "encoded = pe(x)\n",
    "\n",
    "print(f\"\\nPosition signals (first 8 dims of each position):\")\n",
    "for pos in range(6):\n",
    "    vals = ' '.join(f'{v:>6.3f}' for v in encoded[0, pos, :8])\n",
    "    print(f\"  pos {pos}: [{vals} ...]\")\n",
    "print(f\"\\nNotice: each position has a unique pattern.\")\n",
    "print(f\"Lower dims change fast (high freq), higher dims change slow (low freq).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd13752-03c0-47e7-bb2d-419ce47e9aa4",
   "metadata": {},
   "source": [
    "# FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4657149-ecfb-48ce-ad06-d55aa9ede4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = d_model *4\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9fcdb6-b65e-4fa3-97c4-002848bdb075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  torch.Size([1, 4, 64])\n",
      "Inside: (1, 4, 64) → (1, 4, 256) → (1, 4, 64)\n",
      "Output: torch.Size([1, 4, 64])\n",
      "Params: 33,088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ff = FeedForward(d_model=64)\n",
    "x = torch.randn(1, 4, 64)\n",
    "output = ff(x)\n",
    "print(f\"\\nInput:  {x.shape}\")\n",
    "print(f\"Inside: (1, 4, 64) → (1, 4, 256) → (1, 4, 64)\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Params: {sum(p.numel() for p in ff.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6637e185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7394f45e8dd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d05e8e-3bdd-4edb-a4c0-6a9a75226333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16384, 256, 16384, 64]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.numel() for i in ff.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d273ed1-957b-407b-a4f4-a801ab3c36d9",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc252581-81f0-413a-bcab-ab5ebf44a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    \"\"\"\n",
    "        Residual connections (the + x):\n",
    "        Without: output = f(x)                  — gradient must flow through f\n",
    "        With:    output = f(x) + x              — gradient flows directly through +\n",
    "        Same idea as LSTM cell state — provides a highway for gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    # Attention with residuals\n",
    "    normed = self.norm1(x)\n",
    "    att_out, weights = self.attention(normed, mask)\n",
    "    x = x + self.dropout1(att_out) # Residual connection\n",
    "\n",
    "    # Feed Forward with residuals\n",
    "    normed = self.norm2(x)\n",
    "    ff_out = self.ff(normed)\n",
    "    x = x + self.dropout2(ff_out) # Residual connection\n",
    "\n",
    "    return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84214d9b-78b1-4035-837f-227a8d14c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  torch.Size([1, 4, 64])\n",
      "Output: torch.Size([1, 4, 64]) (same shape — residual)\n",
      "Params: 49,728\n"
     ]
    }
   ],
   "source": [
    "block = TransformerBlock(d_model=64, num_heads=8)\n",
    "x = torch.randn(1, 4, 64)\n",
    "output, weights = block(x)\n",
    "print(f\"\\nInput:  {x.shape}\")\n",
    "print(f\"Output: {output.shape} (same shape — residual)\")\n",
    "print(f\"Params: {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75347eee-c58a-4ff1-b9d6-2599bf711db4",
   "metadata": {},
   "source": [
    "# Full transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a04c0702-d25f-4521-bfb7-4b3000675ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer for next-token prediction.\n",
    "\n",
    "    token → embed + position → [block × N] → layernorm → logits\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff=None, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        #Token embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Position encoding\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Stack of transformer block\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Embedding + Scale + Position enc\n",
    "        h = self.token_emb(x) * math.sqrt(self.d_model)\n",
    "        h = self.pos_enc(h)\n",
    "\n",
    "        # Pass through all transformer blocks\n",
    "        all_weights = []\n",
    "        for block in self.blocks:\n",
    "            h, weights = block(h, mask)\n",
    "            all_weights.append(weights)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        return logits, all_weights\n",
    "    \n",
    "    def make_causal_mask(T):\n",
    "        # (1, 1, T, T) — broadcasts over batch and heads\n",
    "        mask = torch.tril(torch.ones(T,T)).unsqueeze(0).unsqueeze(0)\n",
    "        return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb308d92-d8fd-4620-ba0d-5c5d02600d0a",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdcf1da7-e14e-479d-af25-949ba34f2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Same text we used for our scratch RNN\n",
    "    text = \"to be or not to be that is the question. \" * 50\n",
    "\n",
    "    # Build vocab (reusing concept from our RNN)\n",
    "    chars = sorted(set(text))\n",
    "    vocab_size = len(chars)\n",
    "    c2i = {ch: i for i, ch in enumerate(chars)}\n",
    "    i2c = {i: ch for ch, i in c2i.items()}\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  TRAINING TRANSFORMER\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Vocab: {vocab_size} chars: {chars}\")\n",
    "\n",
    "    # Config\n",
    "    d_model = 64*3        # embedding dimension\n",
    "    num_heads = 4       # attention heads\n",
    "    num_layers = 2      # transformer blocks\n",
    "    seq_length = 40\n",
    "    num_epochs = 200\n",
    "    lr = 0.001\n",
    "\n",
    "    # Prepare data (same as RNN)\n",
    "    data = [c2i[ch] for ch in text]\n",
    "    inputs, targets = [], []\n",
    "    for i in range(0, len(data) - seq_length, seq_length):\n",
    "        inputs.append(data[i:i + seq_length])\n",
    "        targets.append(data[i + 1:i + seq_length + 1])\n",
    "\n",
    "    inputs = torch.tensor(inputs)    # (num_batches, T)\n",
    "    targets = torch.tensor(targets)  # (num_batches, T)\n",
    "\n",
    "    # Create model\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        max_len=512, # Changed\n",
    "    )\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {num_layers} layers, {num_heads} heads, d={d_model}\")\n",
    "    print(f\"Params: {total_params:,}\")\n",
    "\n",
    "    # Causal mask (so position i can't see position i+1, i+2, ...)\n",
    "    mask = Transformer.make_causal_mask(seq_length)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx in range(inputs.shape[0]):\n",
    "            x = inputs[batch_idx].unsqueeze(0)     # (1, T)\n",
    "            y = targets[batch_idx].unsqueeze(0)     # (1, T)\n",
    "\n",
    "            logits, _ = model(x, mask)               # (1, T, V)\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.view(-1, vocab_size),\n",
    "                y.view(-1)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / inputs.shape[0]\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
    "            sample = generate(model, c2i, i2c, seed='t', length=80, mask_fn=Transformer.make_causal_mask)\n",
    "            print(f\"Epoch {epoch:>3d} | Loss: {avg_loss:.4f} | {sample}\")\n",
    "\n",
    "    return model, c2i, i2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f6dee95-340f-4ccb-86dd-fdc170021581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. GENERATE TEXT\n",
    "# =============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, c2i, i2c, seed='t', length=100, temperature=0.8, mask_fn=None):\n",
    "    model.eval()\n",
    "    ids = [c2i[ch] for ch in seed]\n",
    "\n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([ids])                       # (1, current_len)\n",
    "        mask = mask_fn(x.size(1)) if mask_fn else None\n",
    "        logits, _ = model(x, mask)                     # (1, current_len, V)\n",
    "\n",
    "        # Take the LAST position's prediction\n",
    "        probs = F.softmax(logits[0, -1] / temperature, dim=0)\n",
    "        idx = torch.multinomial(probs, 1).item()\n",
    "        ids.append(idx)\n",
    "\n",
    "    model.train()\n",
    "    return ''.join(i2c[i] for i in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed634455-f8f7-49e3-b157-3d27b9626bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  TRAINING TRANSFORMER\n",
      "============================================================\n",
      "Vocab: 14 chars: [' ', '.', 'a', 'b', 'e', 'h', 'i', 'n', 'o', 'q', 'r', 's', 't', 'u']\n",
      "Model: 2 layers, 4 heads, d=192\n",
      "Params: 893,966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 1.2306 | tio be t tue thatotor t questistis tor thes tistiuestististististio no istististh\n",
      "Epoch  20 | Loss: 0.2994 | the question. to be or not to to to to to t t to be be to to be t to to t be t to\n",
      "Epoch  40 | Loss: 0.2416 | to be or noto to to to be be to to to to to t that is the question. no noto notot\n",
      "Epoch  60 | Loss: 0.2701 | to to be or noto to t be to to t t t to to t to t t t t t that is the question. q\n",
      "Epoch  80 | Loss: 0.1867 | t to be t be t be be be t be be be be tor be t t be that is the question. not not\n",
      "Epoch 100 | Loss: 0.1879 | tion. to be or noto t to to to to to to be t be be t t be be be be be be be be to\n",
      "Epoch 120 | Loss: 0.2403 | t be or not t be t be to be be to t be be be t be to t t be t be be be be be be t\n",
      "Epoch 140 | Loss: 0.1833 | to be or nothat is the question. tor no no no no no no no no no no no no no no to\n",
      "Epoch 160 | Loss: 0.1799 | t be that is the question. tor be tot tot not no not t not t not nothe totot no t\n",
      "Epoch 180 | Loss: 0.1798 | t that is the question. tor no no notototo nototot nototototot noto nototot notot\n",
      "Epoch 199 | Loss: 0.1995 | to be that is the question. to be be be o be be o o be be be o be be be or totot \n",
      "\n",
      "============================================================\n",
      "  FINAL GENERATION\n",
      "============================================================\n",
      "\n",
      "  temp=0.5: to be that is the question. tor be be be be be be be be be be be be be be be be be notot be be be be be not be be be be be\n",
      "\n",
      "  temp=0.8: to be tor be be be t to t that is the question. to be be to no no t be be be be be be be no to t to no be be be be t be t \n",
      "\n",
      "  temp=1.0: to be tor to be to be be that is the question. to to be be no no t no be be t no be no be be be be be be t no o be t be be\n",
      "\n",
      "============================================================\n",
      "  ATTENTION VISUALIZATION\n",
      "============================================================\n",
      "\n",
      "  Input: 'to be or not'\n",
      "  Layer 0, Head 0 attention pattern:\n",
      "\n",
      "            t     o           b     e           o     r           n     o     t\n",
      "     t → 1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     o → 1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "       → 1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     b → 0.00  0.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     e → 0.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "       → 0.00  0.00  0.00  0.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     o → 1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     r → 0.00  0.00  0.00  0.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "       → 0.00  0.00  0.00  0.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     n → 0.00  0.00  0.34  0.00  0.00  0.32  0.00  0.00  0.34  0.00  0.00  0.00\n",
      "     o → 1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
      "     t → 0.00  0.44  0.00  0.00  0.00  0.00  0.36  0.00  0.00  0.00  0.20  0.00\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RUN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, c2i, i2c = train()\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  FINAL GENERATION\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    for temp in [0.5, 0.8, 1.0]:\n",
    "        text = generate(model, c2i, i2c, seed='to', length=120,\n",
    "                       temperature=temp, mask_fn=Transformer.make_causal_mask)\n",
    "        print(f\"\\n  temp={temp}: {text}\")\n",
    "\n",
    "    # Show attention pattern\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  ATTENTION VISUALIZATION\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    test = \"to be or not\"\n",
    "    ids = [c2i[ch] for ch in test]\n",
    "    x = torch.tensor([ids])\n",
    "    mask = Transformer.make_causal_mask(len(ids))\n",
    "\n",
    "    model.eval()\n",
    "    _, all_weights = model(x, mask)\n",
    "\n",
    "    print(f\"\\n  Input: '{test}'\")\n",
    "    print(f\"  Layer 0, Head 0 attention pattern:\\n\")\n",
    "\n",
    "    chars = list(test)\n",
    "    print(f\"         {'  '.join(f'{c:>4}' for c in chars)}\")\n",
    "    w = all_weights[0][0, 0]  # layer 0, batch 0, head 0\n",
    "    for i, c in enumerate(chars):\n",
    "        vals = '  '.join(f'{v:>4.2f}' for v in w[i, :len(chars)])\n",
    "        print(f\"  {c:>4} → {vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea9546-bdfe-4062-81a5-34582a28e0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b6212-bbce-438a-bb4b-53aca474b856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25ba5e-0436-4a64-91e1-666f21b63ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb97da-70a3-4b7b-9a1a-a0d427d753f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c39e30-c0b9-4868-9816-5768286e1ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
